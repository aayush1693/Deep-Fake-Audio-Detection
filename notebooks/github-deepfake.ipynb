{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Exploration\n",
    "Load the audio files from the training, validation, and testing directories, separating real and fake samples. Normalize the audio data to a consistent format (e.g., sampling rate, bit depth). Augment the dataset to increase diversity (e.g., time stretching, pitch shifting, noise addition). Split the dataset into training, validation, and testing sets. Visualize audio waveforms, spectrograms, and MFCCs for both real and fake audio samples. Analyze the statistical properties of the dataset (e.g., mean, standard deviation, distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download and extract the dataset\n",
    "!wget https://www.eecs.yorku.ca/~bil/Datasets/for-rerec.tar.gz\n",
    "with tarfile.open('for-rerec.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "# Define paths to the dataset directories\n",
    "train_real_dir = 'for-rerecorded/training/real'\n",
    "train_fake_dir = 'for-rerecorded/training/fake'\n",
    "val_real_dir = 'for-rerecorded/validation/real'\n",
    "val_fake_dir = 'for-rerecorded/validation/fake'\n",
    "test_real_dir = 'for-rerecorded/testing/real'\n",
    "test_fake_dir = 'for-rerecorded/testing/fake'\n",
    "\n",
    "# Function to load audio files\n",
    "def load_audio_files(directory):\n",
    "    audio_files = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.wav') or filename.endswith('.mp3'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            audio, sr = librosa.load(filepath, sr=None)\n",
    "            audio_files.append(audio)\n",
    "            labels.append(1 if 'real' in directory else 0)\n",
    "    return audio_files, labels\n",
    "\n",
    "# Load real and fake audio files\n",
    "train_real_audio, train_real_labels = load_audio_files(train_real_dir)\n",
    "train_fake_audio, train_fake_labels = load_audio_files(train_fake_dir)\n",
    "val_real_audio, val_real_labels = load_audio_files(val_real_dir)\n",
    "val_fake_audio, val_fake_labels = load_audio_files(val_fake_dir)\n",
    "test_real_audio, test_real_labels = load_audio_files(test_real_dir)\n",
    "test_fake_audio, test_fake_labels = load_audio_files(test_fake_dir)\n",
    "\n",
    "# Combine real and fake audio data\n",
    "train_audio_data = train_real_audio + train_fake_audio\n",
    "train_labels = train_real_labels + train_fake_labels\n",
    "val_audio_data = val_real_audio + val_fake_audio\n",
    "val_labels = val_real_labels + val_fake_labels\n",
    "test_audio_data = test_real_audio + test_fake_audio\n",
    "test_labels = test_real_labels + test_fake_labels\n",
    "\n",
    "# Normalize audio data\n",
    "def normalize_audio(audio, target_sr=16000):\n",
    "    return librosa.resample(audio, orig_sr=librosa.get_samplerate(audio), target_sr=target_sr)\n",
    "\n",
    "normalized_train_audio = [normalize_audio(audio) for audio in train_audio_data]\n",
    "normalized_val_audio = [normalize_audio(audio) for audio in val_audio_data]\n",
    "normalized_test_audio = [normalize_audio(audio) for audio in test_audio_data]\n",
    "\n",
    "# Data augmentation functions\n",
    "def time_stretch(audio, rate=1.1):\n",
    "    return librosa.effects.time_stretch(audio, rate)\n",
    "\n",
    "def pitch_shift(audio, sr, n_steps=2):\n",
    "    return librosa.effects.pitch_shift(audio, sr, n_steps)\n",
    "\n",
    "def add_noise(audio, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    return audio + noise_factor * noise\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_train_audio = []\n",
    "augmented_train_labels = []\n",
    "for audio, label in zip(normalized_train_audio, train_labels):\n",
    "    augmented_train_audio.append(audio)\n",
    "    augmented_train_labels.append(label)\n",
    "    augmented_train_audio.append(time_stretch(audio))\n",
    "    augmented_train_labels.append(label)\n",
    "    augmented_train_audio.append(pitch_shift(audio, sr=16000))\n",
    "    augmented_train_labels.append(label)\n",
    "    augmented_train_audio.append(add_noise(audio))\n",
    "    augmented_train_labels.append(label)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(augmented_train_audio, augmented_train_labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Visualize audio waveforms, spectrograms, and MFCCs\n",
    "def visualize_audio(audio, sr=16000):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    librosa.display.waveshow(audio, sr=sr)\n",
    "    plt.title('Waveform')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    librosa.display.specshow(mfccs, sr=sr, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample of real and fake audio\n",
    "print(\"Real Audio Sample:\")\n",
    "visualize_audio(X_train[0])\n",
    "\n",
    "print(\"Fake Audio Sample:\")\n",
    "visualize_audio(X_train[len(train_real_audio)])\n",
    "\n",
    "# Analyze statistical properties of the dataset\n",
    "def analyze_statistics(audio_data):\n",
    "    lengths = [len(audio) for audio in audio_data]\n",
    "    mean_length = np.mean(lengths)\n",
    "    std_length = np.std(lengths)\n",
    "    print(f\"Mean length: {mean_length}\")\n",
    "    print(f\"Standard deviation of length: {std_length}\")\n",
    "    plt.hist(lengths, bins=50)\n",
    "    plt.title('Distribution of Audio Lengths')\n",
    "    plt.xlabel('Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "analyze_statistics(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Utilize MobileNet as the backbone for feature extraction. Extract relevant audio features such as Mel-Frequency Cepstral Coefficients (MFCCs), spectral centroids, and chromagrams. Experiment with different window sizes, hop lengths, and MFCC coefficient counts to optimize feature extraction. Visualize the extracted features to understand their discriminative power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D\n",
    "\n",
    "# Function to extract MFCCs\n",
    "def extract_mfcc(audio, sr=16000, n_mfcc=13, hop_length=512, n_fft=2048):\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "    return mfccs\n",
    "\n",
    "# Function to extract spectral centroids\n",
    "def extract_spectral_centroids(audio, sr=16000, hop_length=512):\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=hop_length)\n",
    "    return spectral_centroids\n",
    "\n",
    "# Function to extract chromagrams\n",
    "def extract_chromagram(audio, sr=16000, hop_length=512):\n",
    "    chromagram = librosa.feature.chroma_stft(y=audio, sr=sr, hop_length=hop_length)\n",
    "    return chromagram\n",
    "\n",
    "# Extract features from the dataset\n",
    "def extract_features(audio_data):\n",
    "    mfcc_features = []\n",
    "    spectral_centroid_features = []\n",
    "    chromagram_features = []\n",
    "    \n",
    "    for audio in audio_data:\n",
    "        mfcc_features.append(extract_mfcc(audio))\n",
    "        spectral_centroid_features.append(extract_spectral_centroids(audio))\n",
    "        chromagram_features.append(extract_chromagram(audio))\n",
    "    \n",
    "    return mfcc_features, spectral_centroid_features, chromagram_features\n",
    "\n",
    "mfcc_features, spectral_centroid_features, chromagram_features = extract_features(X_train)\n",
    "\n",
    "# Visualize extracted features\n",
    "def visualize_features(mfcc, spectral_centroid, chromagram):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    librosa.display.specshow(mfcc, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.semilogy(spectral_centroid.T, label='Spectral Centroid')\n",
    "    plt.ylabel('Hz')\n",
    "    plt.xticks([])\n",
    "    plt.xlim([0, spectral_centroid.shape[-1]])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Spectral Centroid')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    librosa.display.specshow(chromagram, y_axis='chroma', x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('Chromagram')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample of extracted features\n",
    "visualize_features(mfcc_features[0], spectral_centroid_features[0], chromagram_features[0])\n",
    "\n",
    "# Utilize MobileNet as the backbone for feature extraction\n",
    "def create_mobilenet_feature_extractor(input_shape):\n",
    "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Example usage of MobileNet feature extractor\n",
    "input_shape = (128, 128, 3)  # Example input shape, adjust as needed\n",
    "mobilenet_model = create_mobilenet_feature_extractor(input_shape)\n",
    "\n",
    "# Convert MFCC features to the shape expected by MobileNet\n",
    "def preprocess_mfcc_for_mobilenet(mfcc_features):\n",
    "    processed_features = []\n",
    "    for mfcc in mfcc_features:\n",
    "        mfcc_resized = np.resize(mfcc, input_shape[:2])\n",
    "        mfcc_3d = np.stack((mfcc_resized,)*3, axis=-1)  # Convert to 3 channels\n",
    "        processed_features.append(mfcc_3d)\n",
    "    return np.array(processed_features)\n",
    "\n",
    "processed_mfcc_features = preprocess_mfcc_for_mobilenet(mfcc_features)\n",
    "\n",
    "# Extract features using MobileNet\n",
    "mobilenet_features = mobilenet_model.predict(processed_mfcc_features)\n",
    "\n",
    "# Visualize the extracted MobileNet features\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(mobilenet_features[0])\n",
    "plt.title('MobileNet Extracted Features')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Feature Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "Design a suitable classification model (e.g., CNN, RNN, or a hybrid) to classify audio samples as real or fake. Train the model on the extracted features. Implement appropriate loss functions (e.g., cross-entropy loss) and optimization algorithms (e.g., Adam). Fine-tune hyperparameters (e.g., learning rate, batch size, epochs) to optimize performance. Evaluate the model's performance using metrics such as accuracy, precision, recall, F1-score, and AUC-ROC. Visualize the model's learning curve, confusion matrix, and ROC curve. Incorporate XAI techniques (e.g., LIME, SHAP) to understand the model's decision-making process. Visualize feature importance and model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the model architecture\n",
    "def create_classification_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = (128, 128, 3)\n",
    "model = create_classification_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Preprocess validation and test data\n",
    "processed_mfcc_features_val = preprocess_mfcc_for_mobilenet(extract_features(X_val)[0])\n",
    "processed_mfcc_features_test = preprocess_mfcc_for_mobilenet(extract_features(X_test)[0])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(processed_mfcc_features, np.array(y_train), validation_data=(processed_mfcc_features_val, np.array(y_val)), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(processed_mfcc_features_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "precision = precision_score(y_test, y_pred_classes)\n",
    "recall = recall_score(y_test, y_pred_classes)\n",
    "f1 = f1_score(y_test, y_pred_classes)\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "# Visualize the learning curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "# Save and download the model\n",
    "model.save('deepfake_audio_detection_model.h5')\n",
    "from google.colab import files\n",
    "files.download('deepfake_audio_detection_model.h5')\n",
    "\n",
    "# XAI Integration using LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(processed_mfcc_features, feature_names=[f'feature_{i}' for i in range(processed_mfcc_features.shape[1])], class_names=['Fake', 'Real'], verbose=True, mode='classification')\n",
    "\n",
    "# Explain a prediction\n",
    "i = 0  # Index of the sample to explain\n",
    "exp = explainer.explain_instance(processed_mfcc_features_test[i], model.predict, num_features=10)\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment and Inference\n",
    "Develop a user interface (e.g., web app, command-line tool) to allow users to input audio files. Preprocess the input audio. Extract features using the trained model. Make a prediction (real or fake). Generate a spectrogram of the input audio. Display the prediction score and classification. Save and download the model. Test the model in Colab itself. Write the main function. Use the downloaded model with a fullstack web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('deepfake_audio_detection_model.h5')\n",
    "\n",
    "# Function to preprocess input audio\n",
    "def preprocess_input_audio(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    mfcc_resized = np.resize(mfcc, (128, 128))\n",
    "    mfcc_3d = np.stack((mfcc_resized,)*3, axis=-1)\n",
    "    return np.array([mfcc_3d])\n",
    "\n",
    "# Function to generate spectrogram\n",
    "def generate_spectrogram(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.savefig('spectrogram.png')\n",
    "    plt.close()\n",
    "    return 'spectrogram.png'\n",
    "\n",
    "# Function to make prediction\n",
    "def predict_audio(audio_path):\n",
    "    processed_audio = preprocess_input_audio(audio_path)\n",
    "    prediction = model.predict(processed_audio)\n",
    "    prediction_score = prediction[0][0]\n",
    "    classification = 'Real' if prediction_score > 0.5 else 'Fake'\n",
    "    spectrogram_path = generate_spectrogram(audio_path)\n",
    "    return classification, prediction_score, spectrogram_path\n",
    "\n",
    "# Create Gradio interface\n",
    "def gradio_interface(audio):\n",
    "    classification, prediction_score, spectrogram_path = predict_audio(audio.name)\n",
    "    return classification, prediction_score, spectrogram_path\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.inputs.Audio(source=\"upload\", type=\"file\"),\n",
    "    outputs=[\n",
    "        gr.outputs.Textbox(label=\"Classification\"),\n",
    "        gr.outputs.Textbox(label=\"Prediction Score\"),\n",
    "        gr.outputs.Image(type=\"file\", label=\"Spectrogram\")\n",
    "    ],\n",
    "    title=\"Deepfake Audio Detection\",\n",
    "    description=\"Upload an audio file to detect if it is real or fake.\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch()\n",
    "\n",
    "# Main function to test the model in Colab\n",
    "def main():\n",
    "    # Test with a sample audio file\n",
    "    sample_audio_path = 'path_to_sample_audio.wav'  # Replace with actual path\n",
    "    classification, prediction_score, spectrogram_path = predict_audio(sample_audio_path)\n",
    "    print(f\"Classification: {classification}\")\n",
    "    print(f\"Prediction Score: {prediction_score}\")\n",
    "    ipd.display(ipd.Audio(sample_audio_path))\n",
    "    img = plt.imread(spectrogram_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
